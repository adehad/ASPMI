%Pre-amble to import suitable modules for a report
\documentclass[12pt]{article}
%\documentclass[draft, 12pt]{article}
\usepackage[utf8]{inputenc}
%\usepackage[]{geometry}
\usepackage[a4paper, left=15mm, right=15mm, top=20mm, bottom=20mm]{geometry}
\usepackage[]{csvsimple}
\usepackage{graphicx}
\usepackage{minipage-marginpar}
\usepackage{enumitem}% http://ctan.org/pkg/enumitem
\setlist[itemize]{itemsep=-0.5ex,after=\vspace{\baselineskip}, topsep=-0.5ex} % requires enumitem package 
\setlist[enumerate]{itemsep=-0.5ex,after=\vspace{\baselineskip}, topsep=-0.5ex} % requires enumitem package 
\usepackage{xcolor} % allows defining your own colours
\usepackage[]{float}
\usepackage[]{caption}
\usepackage[titletoc]{appendix}
\usepackage{chngcntr} % Make nice counter macros availalble
	\counterwithin{figure}{subsection} % Add Section Number to Figure Counter
	\counterwithin{table}{subsection} % Add Section Number to Table Counter
\usepackage{tocbibind}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage[]{multirow}
\usepackage{array} % to allow vertical alignment in tables
\usepackage{makecell} % allows multi-line tables https://tex.stackexchange.com/questions/2441/how-to-add-a-forced-line-break-inside-a-table-cell
\usepackage{pgfplots}
\usepackage{pgfplotstable,filecontents} % enable import of csv files
\pgfplotsset{compat=1.8} % supress a warning or something
\usepackage{wrapfig} % allows figures to have text flow around it
\usepackage{subcaption}
\usepackage{amsmath,amssymb,amsfonts,mathrsfs}  % le math
	\numberwithin{equation}{section}
\usepackage{bm} % bold math symbols
\allowdisplaybreaks
%\usepackage{lscape}
\usepackage{pdflscape} % enables insertion of pdf in landscape
\usepackage{color, colortbl}
	\definecolor{myPink}{cmyk}{0, 0.62, 0.47, 0}
    \definecolor{myRed1}{cmyk}{0, 0.89, 0.54, 0}    \definecolor{myRed2}{cmyk}{0, 0.89, 0.89, 0}
\usepackage{hyperref} % uncomment after document is finished to hyperlink everything
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={green!30!black},
    urlcolor={blue!80!black}
}
\usepackage[export]{adjustbox} % enables setting figure height/width dimensions

%Change these dependent on report - including the main details - this will change the details in titlepage2.tex
\newcommand{\reporttitle}{Coursework}
\newcommand{\reportsupervisor}{Prof. Danilo P. Mandic}
\newcommand{\reporttype}{EE4-13 Adaptive Signal Processing and Machine Intelligence (2018-2019)}
\newcommand{\wordcount}{0}
\date{12th April 2019}

\renewcommand\thesection{\arabic{section}} % Define alphabetical numeration vs. arabic numbering
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsection{\thesubsection.\alph{subsubsection}}

% Expectation & Variance symbol
\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\Var}{\mathbb{V}}
% Correlation
\DeclareMathOperator{\Corr}{Corr}
% Some vectors
\def\vf{{\bm{f}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}
\def\vw{{\bm{w}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}

\def\mF{{\bm{F}}}
% Sets
\def\setInteger{{\mathbb{Z}}}
\def\setReal{{\mathbb{R}}}
\def\setComplex{{\mathbb{C}}}
\def\setNatural{{\mathbb{N}}}





\begin{document}
	
%\setlength{\abovedisplayskip}{3pt}% % Default: 12pt plus 3pt minus 9pt
%\setlength{\belowdisplayskip}{3pt}% % Default: 0pt plus 3pt
%\setlength{\abovedisplayshortskip}{3pt}% % Default: 12pt plus 3pt minus 9pt
%\setlength{\belowdisplayshortskip}{3pt}% % Default: 7pt plus 3pt minus 4pt

\input{titlepage2}
%\maketitle

\newpage
\setcounter{tocdepth}{2} % only show 2 levels in the table of contents
\tableofcontents
%		% 				trim={<left> <lower> <right> <upper>}
\newpage
\section{Classical and Modern Spectrum Estimation} \label{sec: 1-CMSE}
	\subsection{Properties of Power Spectral Density (PSD)} \label{sec: 1-1-prop-PSD}
	
	\subsubsection{Approximation in the Definition of PSD} \label{sec: 1-1a-prop-PSD}
	Starting with the equation provided, (\ref{proof: 1-1a-periodogram:starter}), we: use the relationship between modulus and complex conjugate for complex numbers, we move out the summation terms from the expectation operator, we factor out the exponential terms - as they are independent of the random variable $x$ and we finally use the following substitution:  $g(\tau) = r_{xx}(\tau) e^{-j\omega\tau}$.
	\begin{align}
		P(\omega)   & =\lim_{N\to\infty} \E \bigg\{ \frac{1}{N} \bigg| \sum_{n=0}^{N-1} x(n) e^{-j\omega n} \bigg|^{2} \bigg\}
		\label{proof: 1-1a-periodogram:starter}\\
		& = \lim_{N\to\infty} \E \bigg\{\frac{1}{N}
		\sum_{m=0}^{N-1} x(m) e^{-j\omega m} \sum_{k=0}^{N-1} x^{*}(k) e^{j\omega k} \bigg\}\nonumber\\
		& = \lim_{N\to\infty} \frac{1}{N}
		\sum_{m=0}^{N-1} \sum_{k=0}^{N-1} \E \bigg\{ x(m) e^{-j\omega m} x^{*}(k) e^{j\omega k} \bigg\}\nonumber\\
		& = \lim_{N\to\infty} \frac{1}{N}
		\sum_{m=0}^{N-1} \sum_{k=0}^{N-1} \E \bigg\{ x(m) x^{*}(k) \bigg\} e^{-j\omega(m-k)} \nonumber\\
		& = \lim_{N\to\infty} \frac{1}{N}
		\sum_{m=0}^{N-1} \sum_{k=0}^{N-1} r_{xx}(m-k) e^{-j\omega(m-k)}
		= \lim_{N\to\infty} \frac{1}{N}
		\sum_{m=0}^{N-1} \sum_{k=0}^{N-1} g(m-k)
		\label{proof: 1-1a-periodogram}
	\end{align}
	\\
	We can convert the double summation into a single summation using Equation \ref{proof: 1-1a-periodogram:helper}, and recalling the earlier substitution:
	\begin{equation}
		\sum_{m=-N}^{N} \sum_{k=-N}^{N} g(m-k) = \sum_{\tau=-2N}^{2N}(2N + 1 - |\tau|)g(\tau)
		\label{proof: 1-1a-periodogram:helper}
	\end{equation}
	(\ref{proof: 1-1a-periodogram}) can then be written as:
	\vspace*{-0.8\baselineskip}
	\begin{align}
		P(\omega)    & =         \lim_{N\to\infty} \frac{1}{N} \sum_{\tau=-(N-1)}^{N-1}(N - |\tau|)r_{xx}(\tau) e^{-j\omega\tau}\nonumber\\
		& =         \lim_{N\to\infty} \sum_{\tau=-(N-1)}^{N-1} r_{xx}(\tau) e^{-j\omega\tau} -
		\lim_{N\to\infty} \frac{1}{N} |\tau| \sum_{\tau=-(N-1)}^{N-1} r_{xx}(\tau) e^{-j\omega\tau}\nonumber\\
		& \approx   \sum_{\tau=-\infty}^{\infty} r_{xx}(\tau) e^{-j\omega\tau}
		\label{proof: 1-1a-periodogram:shown}
	\end{align}
	
	\subsubsection{Simulation of the Limiting Case} \label{sec: 1-1b-prop-PSD}
	
	An unbiased estimator of the autocorrelation will provide a case where the correlation does not rapidly decay. This would violate the derivation shown in Section \ref{sec: 1-1a-prop-PSD}, thereby hindering equivalence.
	An example is shown in Figure \ref{fig: 1-1b}, here we see a sine function with an unbiased estimator:
	
	\begin{figure}[H]
		\centering
		\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=0.5\textwidth]{../MATLAB/figures/q1_1b_fig01.pdf} 
		\captionsetup{justification=centering}
		\caption{An example of the Limiting Case of the Periodogram Definition}
		\label{fig: 1-1b}
	\end{figure}
	
	
	\subsection{Periodogram-based Methods Applied to Realâ€“World Data} \label{sec: 1-2-PSD-real-world}
	\subsubsection{The SunSpot Dataset}
	Figure \ref{fig: 1-2a}. The mean's influence on data is the offset DC bias, captured in the $f=0$ component of the periodogram. Hence as we would expect, subtracting the \texttt{mean} reduces its magnitude in the periodogram. \texttt{detrend} removes linear trends, it seems in the case of this data set that most linear trends are captured at $f \lessapprox 0.02 rad/sample$. \\
	
	The natural logarithm was taken using: \texttt{log}. As the logarithm has a compression effect on magnitude we see that the magnitude of both raw and periodogram is greatly attenuated. We note that frequencies of interest and its harmonics appear as more distinct when compared to the rest of the periodogram.

	\begin{figure}[H]
		\centering
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_2a_fig01.pdf} 
			\captionsetup{justification=centering}
			\caption{Raw and its preprocessed datas}
		\end{subfigure}
%		~ % forces onto the same row
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_2a_fig02.pdf} 
			\captionsetup{justification=centering}
			\caption{Periodograms}
		\end{subfigure}
		\captionsetup{justification=centering}
		\caption{}
		\label{fig: 1-2a}
	\end{figure}
%	\begin{figure}[H]
%		\centering
%		%\begin{subfigure}[t]{0.4\textwidth}
%		%	\centering
%		%\hspace*{-2.1cm}
%		% 				trim={<left> <lower> <right> <upper>}
%		\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=9cm]{../MATLAB/figures/q1_2b_fig01.pdf} 
%		%	\captionsetup{justification=centering}
%		%	\caption{Vertical gel electrophoresis setup} 
%		%	\label{fig: verticalGel}
%		%\end{subfigure}
%		%\hfill
%		\captionsetup{justification=centering}
%		\caption{Science being done here}
%	\end{figure}


	\subsubsection{The EEG Dataset}
	
	\begin{figure}[H]
		\centering
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_2b_fig01.pdf} 
			\captionsetup{justification=centering}
			\caption{Standard Periodogram}
		\end{subfigure}
		%		~ % forces onto the same row
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_2b_fig02.pdf} 
			\captionsetup{justification=centering}
			\caption{Bartlett Average Periodograms}
		\end{subfigure}
		\captionsetup{justification=centering}
		\caption{}
		\label{fig: 1-2b}
	\end{figure}

	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c||c|}
			\hline
			\textbf{Response} & \textbf{Expected Range} (Hz) & \textbf{Observed Range} (Hz) \\
			\hline
			{Alpha Rhythm} & $8 - 10$ & $8-10$ \\
			\hline
			{SSVEP} & \texttt{range}[$11-20$] & $13n$ \\
			\hline
			{Power-Line} & $50$ & $50$ \\
			\hline
		\end{tabular}
		\captionsetup{justification=centering}
		\caption{EEG Frequency Peaks of the Periodogram. $n$ refers to harmonics}
		\label{tab: 1-2b}
	\end{table}
	
	The standard periodogram has identifiable peaks, showing in Figure \ref{fig: 1-2b}, quantified in Table \ref{tab: 1-2b}. We are unable to identify the 3rd harmonic of the SSVEP, at 52Hz it is too close to the power-line interference at 50Hz. The main difference in the 10s window averaged periodogram is clearer peak isolation compared to the surrounding periodogram and emphasis on the range of frequencies of the alpha-rhythm, rather than a single discrete peak.

	\subsection{Correlation Estimation} \label{sec: 1-3-correlation-est}
	
	\subsubsection{Unbiased and Biased ACF Estimates}
	
	\begin{figure}[H]
		\centering
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_3a_fig04.pdf} 
		\end{subfigure}
		%		~ % forces onto the same row
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_3a_fig05.pdf} 
		\end{subfigure}
				\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_3a_fig06.pdf} 
		\end{subfigure}
		%		~ % forces onto the same row
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_3a_fig01.pdf} 
		\end{subfigure}
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_3a_fig02.pdf} 
		\end{subfigure}
		%		~ % forces onto the same row
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_3a_fig03.pdf} 
		\end{subfigure}
		\captionsetup{justification=centering}
		\caption{Set of Auto-Correlation Functions (ACFs) and their Correlograms}
		\label{fig: 1-3a}
	\end{figure}
	
	Figure \ref{fig: 1-3a}. For the autocorrelation functions: we can see that the biased estimator tends to 0 for increasing lag magnitude, whereas the unbiased estimator remains somewhat constant, although at the extremes it begins to increase to approximately double the constant value.\\
	For the correlograms: we observe that the biased estimator does not contain negative values and that the unbiased estimator is approximately similar for smaller lags.
	
	\subsubsection{Biased ACF Estimator PSDs}

	The process simulated was the following:
	\begin{equation}
		\begin{aligned}
		x(n) = & 2 sin(2 \pi 0.4 n) + 1.75 sin(2 \pi 0.6 n) \\
		&  + 0.85 sin(2 \pi 0.85 n) + 1.2 sin(2 \pi 0.95 n) + \eta(n) \quad \eta \sim \mathcal{N}(0, 1)
		\end{aligned}
	\end{equation}
	
	

	\begin{figure}[H]
		\centering
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_3b_fig01.pdf} 
			\captionsetup{justification=centering}
			\caption{Periodogram}
		\end{subfigure}
		%		~ % forces onto the same row
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_3b_fig02.pdf} 
			\captionsetup{justification=centering}
			\caption{Standard Deviation}
		\end{subfigure}
		\captionsetup{justification=centering}
		\caption{The total number of data points used was 512, black vertical lines indicate the model defined frequencies}
		\label{fig: 1-3b}
	\end{figure}

	It is interesting to see the low frequency resolution influences the accuracy of the peak with respect to the actual frequencies used, Figure \ref{fig: 1-3b}.

	\subsubsection{Biased ACF Estimator PSDs on the dB Scale}

	\begin{figure}[H]
		\centering
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_3c_fig01.pdf} 
			\captionsetup{justification=centering}
			\caption{Periodogram}
		\end{subfigure}
		%		~ % forces onto the same row
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_3c_fig02.pdf} 
			\captionsetup{justification=centering}
			\caption{Standard Deviation}
		\end{subfigure}
		\captionsetup{justification=centering}
		\caption{The total number of data points used was 512, black vertical lines indicate the model defined frequencies}
		\label{fig: 1-3c}
	\end{figure}

	Figure \ref{fig: 1-3c}. It is advantageous that the standard deviation decreases around our frequencies as be seen at the 4th frequency, where on Figure \ref{fig: 1-3b}, the peak was ambiguous, here as a trough it is much better defined.

	\subsubsection{Influence of Data Samples on the PSD}
	
	\begin{figure}[H]%{R}{0.49\textwidth}
%	\begin{figure}[H]
		\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=0.49\textwidth]{../MATLAB/figures/q1_3d_fig01.pdf} 
%		\end{centering}
%	\end{figure}
	\captionsetup{justification=centering}
	\caption{PSD while varying $n$, the number of Data Samples used}
	\label{fig: 1-3d}
	\end{figure}
	
	In Figure \ref{fig: 1-3d} we can clearly see that the frequency resolution is insufficient at lower sample numbers, resulting in aliasing of the desired frequency peaks.
	
	\subsubsection{MUltiple SIgnal Classification (MUSIC) Estimator}
	
	\begin{figure}[H]
		\centering
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_3e_fig01.pdf} 
		\end{subfigure}
		%		~ % forces onto the same row
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_3e_fig02.pdf} 
		\end{subfigure}
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_3e_fig03.pdf} 
		\end{subfigure}
		%		~ % forces onto the same row
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_3e_fig04.pdf} 
		\end{subfigure}
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_3e_fig05.pdf} 
		\end{subfigure}
		%		~ % forces onto the same row
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_3e_fig06.pdf} 
		\end{subfigure}
		\captionsetup{justification=centering}
		\caption{Set of Auto-Correlation Functions (ACFs) and their Correlograms. \\ $n$ is the number of samples used, $p$ is the Signal Space Dimensionality}
		\label{fig: 1-3e}
	\end{figure}

	The MUSIC estimate identifies eigenvalues of an autocorrelation matrix in descending magnitude order - which indicate directions of largest variability in the subspace. \\
	The first line creates the 14 dimensional modified correlation matrix $R_{xx}$ for the dataset, which is subsequently used by the MUSIC algorithm with $p$ setting the Signal Subspace Dimensionality in the second line. \\
	The third line plots the Pseudospectrum (PSD Estimate) against the frequency axis. \\
	
	Figure \ref{fig: 1-3e} shows the MUSIC estimate plots for the given equation. The spectrum is not very detailed, peaks are not all sharp, but are clearly distinguishable from the surrounding periodogram. In the general case the MUSIC estimator is a good choice if the signal space dimensionality is known, especially as it works on such a limited set of samples. \\
	
	The periodogram is equally suitable for resolving peaks, but requires more samples for a meaningful resolution. \\
	
	In both periodogram and MUSIC estimators we see that the standard deviation, hence variance increase around the peaks, except when using the log scale of the periodogram, where it decreases.
	
	% TODO: A figure doing a direct comparison for the same signal maybe.
	
	% TODO: Bias and Variance
	
	\subsection{Spectrum of Autoregressive (AR) Processes} \label{sec: 1-4-spectrums-AR}

	
	\subsubsection{Shortcomings of the Unbiased ACF in finding AR Parameters}
	
	As the unbiased estimator allows for negative values, at a computational level it will require more bits to store, especially for larger values.
	
	\subsubsection{Error of the AR PSD Estimate}
	\begin{figure}[H]
		\centering
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_4b_fig14.pdf} 
			\captionsetup{justification=centering}
			\caption{AR Periodogram and its $p$ Order Estimates}
		\end{subfigure}
		%		~ % forces onto the same row
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_4b_fig16.pdf} 
			\captionsetup{justification=centering}
			\caption{Mean Squared Error with two y-scales}
		\end{subfigure}
		\captionsetup{justification=centering}
		\caption{}
		\label{fig: 1-4b}
	\end{figure}

	We can see in (a) of \ref{fig: 1-4b}, that increasing the order tends towards a better solution. But (b) notes that the most drastic difference is at the model order, 4, which matches the order of the process defined, subsequent increases of the model order do increase its likeliness to the true response, but changes are not so drastic.
	

	\subsubsection{Error of the AR PSD Estimate with more Samples}
	\begin{figure}[H]
		\centering
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_4c_fig14.pdf} 
			\captionsetup{justification=centering}
			\caption{AR Periodogram and its $p$ Order Estimates}
		\end{subfigure}
		%		~ % forces onto the same row
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_4c_fig16.pdf} 
			\captionsetup{justification=centering}
			\caption{Mean Squared Error with two y-scales}
		\end{subfigure}
		\captionsetup{justification=centering}
		\caption{}
		\label{fig: 1-4c}
	\end{figure}

	The same trend is seen in Figure \ref{fig: 1-4c}, as we saw with $N=500$, although the estimate now matches the underlying model much better, reflected in the drastically lower Mean Square Error (MSE). \\
	
	It is noted that for a more valid comparison of model order's influence on the estimate's error the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) are more suitable quantifiers of model order suitability than MSE. But the accuracy trend reflected in the MSE is still valid for our comparisons.
	
	\pagebreak
	
	\subsection{Real World Signals: Respiratory Sinus Arrhythmia from RR-Intervals} \label{sec: 1-5-real-world-signals}
	
	\subsubsection{Standard \& Average PSDs of the RRI Dataset}
	\begin{figure}[H]
		\centering
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_5a_fig01.pdf} 
		\end{subfigure}
		%		~ % forces onto the same row
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_5a_fig04.pdf} 
		\end{subfigure}
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_5a_fig02.pdf} 
		\end{subfigure}
		%		~ % forces onto the same row
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_5a_fig05.pdf} 
		\end{subfigure}
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_5a_fig03.pdf} 
		\end{subfigure}
		%		~ % forces onto the same row
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_5a_fig06.pdf} 
		\end{subfigure}
		\captionsetup{justification=centering}
		\caption{Standard and Bartlett Average Periodograms. \\ 
				$W_L$ is the Window Length used. First 4 harmonics denoted by vertical black lines. Zero Padded Signal Length: $4096$.}
		\label{fig: 1-5a}
	\end{figure}

	\begin{table}[H]
		\centering
		\begin{tabular}{|c|c||c|}
			\hline
			\textbf{Breathing Type} & \textbf{Expected} (BPM) & \textbf{Observed Peak} (BPM) \\
			\hline
			{Normal (Trial 1)} & $10-15$ & $0.31\times60\approx18.7$ \\
			\hline
			{Fast (Trial 2)} & $25$ & $0.41\times60\approx25$ \\
			\hline
			{Slow (Trial 3)} & $7.5$ & $0.125\times60=7.5$ \\
			\hline
		\end{tabular}
		\captionsetup{justification=centering}
		\caption{Breaths Per Minute (BPM), i.e. Observed Frequency $\times 60$, for all trials.}
		\label{tab: 1-5a}
	\end{table}


	\subsubsection{Analysis of the RRI PSD Estimates}
	
	Figure \ref{fig: 1-5a} shows us distinct peaks for each trial that somewhat agrees with the breathing expected. The performance of the $W_L=50$ is also sufficiently accurate for Trials 1 and 2, however less so for Trial 3. We note that harmonics are difficult to distinguish, despite the large zero padding of the signal. Table \ref{tab: 1-5a} highlights how the observed peaks align with the expected peaks.
	\pagebreak
	
	\subsubsection{AR PSD Estimate for the RRI Dataset}
	
%	\begin{wrapfigure}{r}{0.49\textwidth}
%		\vspace{-20pt}
%		\begin{centering}
%			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=0.49\textwidth]{../MATLAB/figures/q1_5c_fig01.pdf} 
%		\end{centering}
%%		\vspace{-20pt}
%%		\vspace{-10pt}
%		\begin{centering}
%			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=0.49\textwidth]{../MATLAB/figures/q1_5c_fig02.pdf} 
%		\end{centering}
%%		\vspace{-20pt}
%%		\vspace{-10pt}
%		\begin{centering}
%			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=0.49\textwidth]{../MATLAB/figures/q1_5c_fig03.pdf} 
%		\end{centering}
%%		\vspace{-20pt}
%		\captionsetup{justification=centering}
%		\caption{AR Estimate Periodograms. \\ $p$ is the model order.}
%		\label{fig: 1-5c}
%%		\vspace{-10pt}
%	\end{wrapfigure}

		\begin{figure}[H]
			\begin{subfigure}{0.49\textwidth}
				\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_5c_fig01.pdf} 
				\caption{}
			\end{subfigure}
			\begin{subfigure}{0.49\textwidth}
				\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_5c_fig02.pdf} 
				\caption{}
			\end{subfigure}%
%			\hspace*{0.25\textwidth}
%			\begin{subfigure}{0.49\textwidth}
%				\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_5c_fig03.pdf} 
%			\end{subfigure}
%			\captionsetup{justification=centering}
%			\caption{AR Estimate Periodograms. \\ $p$ is the model order.}
%			\label{fig: 1-5c}
			%		\vspace{-10pt}
			
			\begin{minipage}{0.46\textwidth}
				The AR spectral estimate in Figure \ref{fig: 1-5c} correctly identified the peak from models of order $p\gtrapprox10$. We note that the estimate resembles a smooth envelope over our PSD and correctly identifies clear peaks in the standard PSD at $p=10$. We also can see for higher order models that there are move fluctuations as the model starts to overfit for the input data's natural noisiness. Compared to our periodogram estimate we are better able to find harmonics in Trial 2 with the AR model, but unfortunately they are not accurate.
			\end{minipage}% 
			\begin{minipage}{0.04\textwidth}
				\hspace*{0.04\textwidth}
			\end{minipage}% 
			\begin{minipage}{0.49\textwidth}
				\begin{subfigure}{\textwidth}
					\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_5c_fig03.pdf} 
					\caption{}
				\end{subfigure}%
%				\centering
%				\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_5c_fig03.pdf} 
				\captionsetup{justification=centering}
				\captionof{figure}{AR Estimate Periodograms. \\ $p$ is the model order. Black lines are frequency of interest and its harmonics}
				\label{fig: 1-5c}
			\end{minipage}%
		\end{figure}
	

%	\pagebreak
	
	
	\subsection{Robust Regression} \label{sec: 1-6-robust-regression} 
	 	\subsubsection{Single Value Decomposition (SVD)}
			\begin{figure}[H]
				\centering
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_6a_fig01.pdf} 
					\captionsetup{justification=centering}
					\caption{SVD}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_6a_fig02.pdf} 
					\captionsetup{justification=centering}
					\caption{Square Error}
				\end{subfigure}
				\captionsetup{justification=centering}
				\caption{}
				\label{fig: 1-6a}
			\end{figure}
		
		
		
			The rank of the input data would be indicated when the SVD Error increases or the non-zero SVD values end (for pure data) - hence the data is of rank 3. \\
			Noise makes the SVD magnitude non-zero where we expect it to be zero for the pure input. Hence, in the real world where we would not have the pure dataset we would need to purely look at Figure \ref{fig: 1-6a} (a), if the SVD magnitude of the noise was comparable to that of the signal we would have difficulty establishing a criteria to distinguish signal and noise subspaces from the SVD method. \\
	 
	% 	\pagebreak
	 	
	 	\subsubsection{Low Rank Approximation Error}
		 	\begin{minipage}[b]{0.49\textwidth}
			By looking at the mean square error between the pure signal and the low-rank approximation of the noise, Figure \ref{fig: 1-6b}, we note that the minimum error is when the low-rank approximation matches that of the pure data. This clear difference would allow automated algorithms to search for the minimum MSE error and correct obtain the appropriate model order.
			\end{minipage}% 
			\begin{minipage}{0.04\textwidth}
				\hspace*{0.04\textwidth}
			\end{minipage}% 
			\begin{minipage}{0.49\textwidth}
				\centering
				\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_6b_fig01.pdf} 
				\captionsetup{justification=centering}
				\captionof{figure}{Effect of Changing Rank on the Approximation Error}
				\label{fig: 1-6b}
			\end{minipage}%
	
	 
	 	\subsubsection{Ordinary Least Squares (OLS) \& Principle Component Regression (PCR)  Estimate Errors}
	 	
		 	\begin{figure}[H]
		 		\centering
		 		\begin{subfigure}{0.49\textwidth}
		 			\centering
		 			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_6c_fig01.pdf} 
		 			\captionsetup{justification=centering}
		 			\caption{Training Dataset Error}
		 		\end{subfigure}
		 		%		~ % forces onto the same row
		 		\begin{subfigure}{0.49\textwidth}
		 			\centering
		 			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_6c_fig02.pdf} 
		 			\captionsetup{justification=centering}
		 			\caption{Testing Dataset Error}
		 		\end{subfigure}
		 		\captionsetup{justification=centering}
		 		\caption{Comparison of Errors in the Training and Testing Datasets}
		 		\label{fig: 1-6c}
		 	\end{figure}
		 
		 	In both datasets we see that at the true model order the differences are tiny, in the training dataset we see that PCR underperforms by $0.85\%$, while in the testing dataset PCR outperforms by $1.05\%$. We can also note that decreasing the PCR model order further than the true model order, improves performance in the Training dataset but reduces performance in the Testing dataset.
		
		
	 	\subsubsection{Ordinary Least Squares (OLS) \& Principle Component Regression (PCR)  Estimate Errors - Part 2}
	 	\begin{minipage}{0.49\textwidth}
	 		Again if we look at the true model order, Figure \ref{fig: 1-6d}, PCR outperforms OLS by $0.79\%$. We note how further reduction in dimensionality decreases the performance of PCR. In regard to effectiveness of each scheme, if the model order is unknown and cannot be approximated accurately, OLS is a safer choice, however best performance - although marginal - can be achieved if the model order is known and PCR is implemented.
	 	\end{minipage}% 
		 \begin{minipage}{0.04\textwidth}
		 	  \hspace*{0.04\textwidth}
		 \end{minipage}% 
	 	\begin{minipage}{0.49\textwidth}
			\centering
	 		\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q1_6d_fig01.pdf} 
	 		\captionsetup{justification=centering}
	 		\captionof{figure}{Mean Square Error over several Realisations}
	 		\label{fig: 1-6d}
	 	\end{minipage}%
 
\pagebreak
\section{Adaptive Signal Processing} \label{sec: 2-ASP}
	\subsection{The Least Mean Square (LMS) Algorithm} \label{sec: 2-1-LMS}
		\subsubsection{Example Correlation Matrix}
			An AR(2) process $x(n)$ with parameters $a_{1}, a_{2}$ satisfies the difference equation:
			\begin{equation}
			x(n) = a_{1} x(n - 1) + a_{2} x(n - 2) + \eta(n)
			\label{eq: 2-1a-ar_2}
			\end{equation}
			
			where $\eta(n) \sim \mathcal{N}(0, \sigma_{\eta}^{2})$. The Least Mean Square (LMS) algorithm is used to approximate the autoregressive parameters $a_{1}, a_{2}$ from data, with input $\mathbf{x}(n) = [x(n-1), x(n-2)]^{T}$ and output
			$y(n) = x(n)$ .
			
			We will first check for stationarity, we know that for an AR(2) process to be stationary the roots of its characteristic polynomial must lie outside the unit circle, which can be confirmed if the following are satisfied:
			\vspace*{-\baselineskip}
			\begin{align}
			a_1+a_2 & < 1 \\
			a_2-a_1 & < 1 \\
			|a_2| & < 1 
			\label{eq: 2-1a-cond:stationary}
			\end{align}
			
			We can confirm is valid for our AR(2) process, which has $a_1=0.1$, $a_2=0.8$.\\
			The correlation matrix $\mathbf{R}_{xx}$ of the stationary input is thereby given by:
			\begin{equation}
			\mathbf{R}_{xx} = \E[\mathbf{x}(n) \mathbf{x}^{T}(n)] = \E
			\begin{bmatrix}
			x(n-1) x(n-1) & x(n-1) x(n-2) \\
			x(n-2) x(n-1) & x(n-2) x(n-2)
			\end{bmatrix} =
			\begin{bmatrix}
			r_{xx}(0) & r_{xx}(1) \\
			r_{xx}(1) & r_{xx}(0)
			\end{bmatrix}
			\label{eq: 2-1a-cov_ar_2}
			\end{equation}
			
			where $r_{xx}(k)$ is the autocorrelation function (ACF) of $x(n)$. To obtain the ACF of the AR(2) process, we must then multiply equation (\ref{eq: 2-1a-ar_2}) by $x(n-k)$ and take the Expectation, $\E[\cdot]$:
			\begin{align}
			r_{xx}(k) &= \E[ x(n) x(n-k) ] \nonumber\\
			  		  &= \E \bigg[ a_{1} x(n - 1) x(n-k) + a_{2} x(n - 2) x(n-k) + \eta(n) x(n-k) \bigg] \nonumber\\
			  		  &= a_{1} \E \big[ x(n - 1) x(n-k) \big] + a_{2} \E \big[ x(n - 2) x(n-k) \big] + \E \big[ \eta(n) x(n-k) \big]
			\end{align}
			
			$\E [\eta(n) x(n-k)]$ goes to $0$ when $k > 0$: 
			\vspace*{-0.2\baselineskip}
			\begin{align}
			r_{xx}(0)   &= a_{1} r_{xx}(1) + a_{2} r_{xx}(2) + \sigma_{\eta}^{2},  && k = 0 \\
			r_{xx}(k)   &= a_{1} r_{xx}(k-1) + a_{2} r_{xx}(k-2),   && k > 0
			\label{eq: 2-1a-acf_ar_2}
			\end{align}
			\noindent
			Using the true process parameters $a_{1} = 0.1$, $a_{2} = 0.8$ and $\sigma_{\eta}^{2} = 0.25$, together with the even property of the ACF, $r_{xx}(-k) = r_{xx}(k), \forall k \in \setNatural$, we obtain three simultaneous equations with three unknowns:
			\vspace*{-\baselineskip}
			\begin{align}
			r_{xx}(0)   &= a_{1} r_{xx}(1) + a_{2} r_{xx}(2) + \sigma_{\eta}^{2} \\
			r_{xx}(1)   &= a_{1} r_{xx}(0) + a_{2} r_{xx}(-1) = a_{1} r_{xx}(0) + a_{2} r_{xx}(1) \\
			r_{xx}(2)   &= a_{1} r_{xx}(1) + a_{2} r_{xx}(0)
			\end{align}
			
			with the unique solution $r_{xx} = [\frac{25}{27}, \frac{25}{54}, \frac{85}{108}]$ for $k=0, 1, 2$. \newline
			\noindent
			Hence by substitution in equation (\ref{eq: 2-1a-cov_ar_2}) we obtain the  correlation matrix of the input vector $\mathbf{x}(n)$:
			\begin{equation}
			\mathbf{R}_{xx} = 
			\frac{25}{54}
			\begin{bmatrix}
			2 & 1 \\
			1 & 2
			\end{bmatrix}
			\label{eq: 2-1a-cov_ar_2_result}
			\end{equation}
			\noindent
			Convergence of the LMS algorithm depends on the step size $\mu$, which should satisfy:
			\begin{equation}
			0 < \mu < \frac{2}{\lambda_{max}}
			\label{eq: 2-1a-cond:mu_max}
			\end{equation}
			
			where $\lambda_{max}$ the largest eigenvalue of the correlation matrix $\mathbf{R}_{xx}$. Performing the eigendecomposition of $\mathbf{R}_{xx}$, we obtain
			$\lambda_{1} = 0.4630$ and $\lambda_{2} = \lambda_{max} = 1.3889$ and as a result, convergence to the Wiener optimal solution is guaranteed for:
			\vspace*{-0.8\baselineskip}
			\begin{equation}
			0 < \mu < 1.44
			\label{eq: 2-1a-cond:mu_max_val}
			\end{equation}
			
		\subsubsection{Example Learning Curve for an AR(2) Process}
		
			\begin{figure}[H]
				\centering
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_1b_fig01.pdf} 
					\captionsetup{justification=centering}
					\caption{Square Prediction Error, Single Realisation}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_1b_fig03.pdf} 
					\captionsetup{justification=centering}
					\caption{Log of Mean of the Squared Prediction Error}
				\end{subfigure}
				\captionsetup{justification=centering}
				\caption{$\mu$'s effect on the Squared Prediction Error and the Influence of the Log operator}
				\label{fig: 2-1b}
			\end{figure}
		
		As can be seen in Figure \ref{fig: 2-1b}, the log operator exponentially emphasises the small trends in the error signal. It is now easier to see the influence of the higher $\mu$ on reaching steady state, where we can see that larger values converge faster.
		
		\subsubsection{Misadjustment}
			
			The theoretical Misadjustment, $\mathcal{M}_{LMS}$, of the LMS algorithm can be approximated with:
			\vspace*{-0.4\baselineskip}
			\begin{equation}
			\mathcal{M}_{LMS} \approx \frac{\mu}{2} Tr\big\{ \mathbf{R}_{xx} \big\} = \frac{\mu}{2} \left(\frac{100}{54}\right) = \frac{\mu}{2} 1.8519
			\end{equation}
			\indent
			where $\mathbf{R}_{xx}$ from (\ref{eq: 2-1a-cov_ar_2_result}) is used. \newline
			\noindent
			Figure \ref{fig: 2-1b} shows that the squared prediction error converges for both $\mu$ after $t > 200$. Looking at $t > 400$ to guarantee that a steady-state has been reached, then a Mean Squared Error (MSE) can be estimated. The empirical misadjustment, $\mathcal{M}_{emp}$, is found with:
%			\vspace*{-0.2\baselineskip}
			\begin{equation}
			\mathcal{M}_{emp} = \frac{\mathtt{EMSE}}{\sigma_{\eta}^{2}} = \frac{\mathtt{MSE}}{\sigma_{\eta}^{2}} -1
			\end{equation}
			\noindent
			Table \ref{tab: 2-1c} summarises the empirical, theoretical and practical misadjustments of the simple LMS algorithm for the $x(n)$ process.
						
			\begin{table}[H]
				\centering
				\begin{tabular}{|c|c|c||c|}
					\hline
					\textbf{$\mu$} & \textbf{$\mathcal{M}_{emp}$} & Theoretical \textbf{$\mathcal{M}_{LMS}$} & Practical \textbf{$\mathcal{M}_{LMS}$} \\
					\hline
					0.01 & $0.0093$ & $0.0077$ & $0.0182 \pm 0.05619$ \\
					\hline
					0.005 & $0.0463$ & $0.0491$ & $0.0593 \pm 0.06409$ \\
					\hline
				\end{tabular}
				\captionsetup{justification=centering}
				\caption{Theoretical and LMS Estimated Misadjustments}
				\label{tab: 2-1c}
			\end{table}
		
			\begin{minipage}[b]{0.49\textwidth}
				Misadjustment for larger $\mu$ values appears to be larger, corresponding to what is observed in Figure \ref{fig: 2-1b} and \ref{fig: 2-1d}, where larger MSE is contributing to this difference. It is clear that the standard deviations of misadjustment are very large in the practical data.
			\end{minipage}% 
			\begin{minipage}{0.04\textwidth}
				\hspace*{0.04\textwidth}
			\end{minipage}% 
			\begin{minipage}[t]{0.49\textwidth}
				\centering
				\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_1cd_fig01.pdf} 
				\captionsetup{justification=centering}
				\captionof{figure}{Estimated Misadjustment, varying with realisation. Theoretical $\mathcal{M}_{LMS}$ is additionally plotted for reference. }
				\label{fig: 2-1c}			
			\end{minipage}%

		\subsubsection{Steady State Estimation} 
			\begin{figure}[H]
				\centering
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_1cd_fig02.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width
					=\textwidth]{../MATLAB/figures/q2_1cd_fig03.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				\captionsetup{justification=centering}
				\caption{$\mu$'s effect on weight stabilisation}
				\label{fig: 2-1d}
			\end{figure}
		The simulation shown in Figure \ref{fig: 2-1d}, shows that convergence to a steady state is faster with larger $\mu$ values, 200 ($\mu=0.05$) vs 900 ($\mu=0.01$), however, we clearly observe the tradeoff where the converged value is less accurate and more noisy, as can be clearly seen for $\hat{a}_1$.
		\subsubsection{Leaky LMS Derivation}
			The cost function $\mathcal{J}_{2}$:
			\vspace*{-0.6\baselineskip}
			\begin{equation}
			\mathcal{J}_{2}(n) = \frac{1}{2} \bigg( e^{2}(n) + \gamma \| \vw(n) \|_{2}^{2} \bigg)
			\label{eq: 2-1e-J_2}
			\end{equation}
			
			Expressing the error term, $e(n)$, in terms of the input vector $\mathbf{x}_{n}$ and the (generic) target $y_{n}$:
			\begin{align}
			\mathcal{J}_{2}(n)  &= \frac{1}{2} \bigg( \| y(n) - \vw(n)^{T} \mathbf{x}(n) \|_{2}^{2} + \gamma \| \vw(n) \|_{2}^{2} \bigg) \\
							    &= \frac{1}{2} \bigg( \big( y(n) - \vw(n)^{T} \mathbf{x}(n))^{T} (y(n) - \vw(n)^{T} \mathbf{x}(n) \big) + \gamma \vw(n)^{T} \vw(n) \bigg)
			\end{align}
			\noindent
			Calculating the gradient of the cost function, $\nabla_{\vw} \mathcal{J}_{2}$, with respect to the weights $\vw$:
			\begin{align}
			\nabla_{\vw} \mathcal{J}_{2}(n) &= \frac{1}{2} \frac{\partial}{\partial\vw}\bigg( \big( y(n) - \vw(n)^{T} \mathbf{x}(n))^{T} (y(n) - \vw(n)^{T} \mathbf{x}(n) \big) + \gamma \vw(n)^{T} \vw(n) \bigg) \\
											&= \frac{1}{2} \big( -2 ( y(n) - \vw(n)^{T} \mathbf{x}(n) ) \mathbf{x}(n) + 2\gamma \vw(n) \big)\\
											&= - ( y(n) - \vw(n)^{T} \mathbf{x}(n) ) \mathbf{x}(n) + \gamma \vw(n) \\
											&= - e(n) \mathbf{x}(n) + \gamma \vw(n)
			\end{align}
			\noindent
			We take the negative of this gradient, i.e. gradient descent, hence the weight update equation becomes:
			\vspace*{-\baselineskip}
			\begin{align}
			\vw(n + 1)  &= \vw(n) - \mu \nabla_{\vw} \mathcal{J}_{2}(n) \\
						&= \vw(n) - \mu \big( - e(n) \mathbf{x}(n) + \gamma \vw(n) \big) \\
						&= (1 - \mu \gamma) \vw(n) + \mu e(n) \mathbf{x}(n)
			\label{eq: 2-1e-leaky_lms}
			\end{align}
			\noindent
			Hence we proved that the Leaky LMS algorithm weight update rule, (\ref{eq: 2-1e-leaky_lms}), is derived from the minimisation of the cost function $\mathcal{J}_{2}$, (\ref{eq: 2-1e-J_2}).
		\subsubsection{Example Leaky LMS for an AR(2) Process}  
		
			\begin{figure}[H]
				\centering
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_1f_fig01.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_1f_fig02.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
			
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_1f_fig04.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_1f_fig05.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
			
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_1f_fig07.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_1f_fig08.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				\captionsetup{justification=centering}
				\caption{$\gamma$'s leakage, forgetful, effect on weight stabilisation}
				\label{fig: 2-1f}
			\end{figure}
		
		The leaky LMS implements the term $\gamma$, which we can interpret as a forgetfulness term, where the LMS filter forgets the previous weight's significance. This means that the filter reaches a steady state faster, which can be observed in Figure \ref{fig: 2-1f}, where we see convergence is 400 steps instead of 900 when using $\gamma=0.5$. \\
		Importantly, the accuracy is greatly compromised, the magnitude of the error is directly proportional to the $\gamma$ used. \\
		The usefulness of the leaky LMS is more apparent when the $\mu$ value is unstable in the original LMS implementation, now the leaky LMS is capable of convergence whereas the original LMS will not converge to a reasonable value.
		 
	\subsection{Adaptive Step Sizes} \label{sec: 2-2-adaptive-step}
		\subsubsection{Gradient Adaptive Step Sizes (GASS) Comparisons for a MA(1) System} 
			\vspace*{-\baselineskip}
			\begin{figure}[H]
				\centering
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_2a_fig03.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_2a_fig04.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				\captionsetup{justification=centering}
				\caption{Weight Error and Prediction Error for various GASS algorithms}
				\label{fig: 2-2a}
			\end{figure}
			In brief, the step size is modulated in: Matthews \& Xie utilises by the gradient of the error, Ang \& Farhang by a static low pass to reduce noise effects and in Benveniste by an adaptive low pass.
			Varying our $\mu$ term would allow us to learn faster, by taking larger steps of change in our weights, for example when the error is large. What we observe in the gradients in the (log emphasised) mean square error plot, Figure \ref{fig: 2-2a}, is the effective learning rate for these algorithms. Additionally we can see the influence of changing the algorithm's starting learning rate, and its effect on convergence. It is clear that the Beneviste algorithm has the faster convergence, but when looking at the effect of increasing $\mu$ we observe the improvement of learning rate (increase of gradient magnitude) in descending order is: LMS, Matthews \& Xie then Ang \& Farhang.\\
			In the highly dynamic applications where such filters would be needed, we can imagine that the learning rate can influence the output properties when compared to the desired output. For example, if the learning rate was less than or equal to the applications fluctuation rate we would observe a low pass filter effect, as the predicted output would not necessarily reach the actual output in the time between fluctuations.
		
		\subsubsection{NLMS Update Equation Equivalence}
			Starting from the update equation based on the \textit{a posteriori error} $e_{p}(n) = d(n) - \mathbf{x}(n)^{T} \vw(n + 1)$:
			\begin{equation}
			\vw(n + 1) = \vw(n) + \mu e_{p}(n) \mathbf{x}(n)
			\end{equation}
			
			Multiply both sides with $-\mathbf{x}(n)^{T}$ and add $d(n)$ to create $e_{p}(n)$ :
			\begin{equation}
			d(n) - \mathbf{x}(n)^{T} \vw(n + 1) = d(n) - \mathbf{x}(n)^{T} \vw(n) - \mathbf{x}(n)^{T} \mu e_{p}(n) \mathbf{x}(n)
			\end{equation}
			
			The LHS term is the \textit{a posteriori} error, $e_{p}(n)$, while the first RHS term the \textit{a priori error}, $e(n)$:
			\begin{align}
			e_{p}(n)    &= e(n) - \mu e_{p}(n) \| \mathbf{x}(n) \|^{2} \\
			e_{p}(n)    &= e(n) \frac{1}{1 + \mu \| \mathbf{x}(n) \|^{2}}
			% &= e(n) \bigg[ \frac{1 + \mu \| \mathbf{x}(n) \|^{2} - \mu \| \mathbf{x}(n) \|^{2} }{1 + \mu \| \mathbf{x}(n) \|^{2}} \bigg] \\
			% &= e(n) \bigg[ 1 - \mu \frac{\| \mathbf{x}(n) \|^{2}}{1 + \mu \| \mathbf{x}(n) \|^{2}} \bigg]
			\label{eq: 2-2b-aa_errors}
			\end{align}
			
			Substituting (\ref{eq: 2-2b-aa_errors}) into the weight update equation:
			\begin{align}
			\vw(n + 1)  &= \vw(n) + \mu \frac{1}{1 + \mu \| \mathbf{x}(n) \|^{2}} e(n) \mathbf{x}(n) \\
			  &= \vw(n) + \frac{1}{\frac{1}{\mu} + \| \mathbf{x}(n) \|^{2}} e(n) \mathbf{x}(n) \label{eq: 2-2b-nlms}
			\end{align}
			\noindent
			Hence, with $\beta = 1$ and $\epsilon = \frac{1}{\mu}$ we have shown that the NLMS algorithm (\ref{eq: 2-2b-nlms}) is equivalent to the weight update equation based on the \textit{a posteriori error}.
		
		\subsubsection{Generalised Normalised Gradient Descent (GNGD) Example for a MA(1) System} 
		
			\begin{figure}[H]
				\centering
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_2c_fig03.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_2c_fig04.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				\captionsetup{justification=centering}
				\caption{Weight Error and Prediction Error comparing the GNGD NLMS to the Benveniste GASS Algorithm}
				\label{fig: 2-2c}
			\end{figure}
		
			As we explored earlier the LMS algorithm's convergence time is closely tied to the value of $\mu$, showing the greatest change for the $\times10$ increase of $\mu$. Figure \ref{fig: 2-2c}, shows that we can surpass the rate of convergence of the Benveniste algorithm when using the NLMS algorithm. The NLMS algorithm allows us to choose large values of $\mu$, that would traditionally cause the output of the LMS to be unstable, due to its GNGD normalisation approach. \\
			Computationally these algorithms are driven by their outer product calculations, the Benveniste algorithm performs 2 per step, whereas the GNGD NLMS algorithm performs 1 per step making it of lower computational complexity.
			
	\subsection{Adaptive Noise Cancellation} \label{sec: 2-3-ANC}
		With the: pure sine wave $x(n)$, noisy signal $s(n)$,  noise term $\eta(n)$ and ALE filter output $\hat{x}(n; \Delta)$, parametrised by the delay parameter $\Delta$.
		Then the Mean Squared Error (MSE) is given by:
		\begin{align}
		\mathtt{MSE} =
		\E \bigg[ \big( s(n) - \hat{x}(n; \Delta) \big)^{2} \bigg]  &=  \E \bigg[ \big( x(n) + \eta(n) - \hat{x}(n; \Delta) \big)^{2} \bigg] \\
		&=  \E \bigg[ \big( \eta(n) + (x(n) - \hat{x}(n; \Delta)) \big)^{2} \bigg] \\
		&=  \E \bigg[ \eta^{2}(n) \bigg] +
		\E \bigg[ \big( x(n) - \hat{x}(n; \Delta) \big)^{2} \bigg] +
		2\E \bigg[ \eta(n) \big(x(n) - \hat{x}(n; \Delta) \big) \bigg]
		\label{eq: 2-3a-ale_mse}
		\end{align}
		\noindent
		The 1st term, noise power $\E [ \eta^{2}(n) ]$, is independent of $\Delta$, the 2nd term, Mean Squared Prediction Error (MSPE) $\E [ (x(n) - \hat{x}(n; \Delta))^{2} ]$ is independent of noise $\eta(n)$, the 3rd term involves both the delay $\Delta$ (through $\hat{x}(n)$) the noise term and a 2 pre-factor, so is a candidate for minimisation:
		\begin{align}
		&\underset{\Delta \in \setNatural}{min}\ \E \bigg[ \eta(n) \big(x(n) - \hat{x}(n; \Delta) \big) \bigg] = \underset{\Delta \in \setNatural}{min}\ \E \bigg[ \eta(n) x(n) \bigg]- \underset{\Delta \in \setNatural}{min}\ \E \bigg[ \eta(n)\hat{x}(n; \Delta) \bigg] \\
		&\textnormal{As $x(n)$ and $\eta(n)$ are uncorrelated, any  $\E [ \eta(n) x(n) ]$ terms go to $0$:} \nonumber\\
		&=
		\underset{\Delta \in \setNatural}{min}\ \E \bigg[ \big( \eta(n) \big) \vw^{T} \vv(n; \Delta) \bigg] \\
		&=
		\underset{\Delta \in \setNatural}{min}\ \E \bigg[ \big( \eta(n) \big) \sum_{i=0}^{M-1} w_{i} s(n - \Delta - i) \bigg] \\
		&=
		\underset{\Delta \in \setNatural}{min}\ \E \bigg[ \big( \eta(n) \big) \sum_{i=0}^{M-1} w_{i} \big( x(n - \Delta - i) + \eta(n - \Delta - i) \big) \bigg] \\
		&\textnormal{Now using the definition of $\eta(n) = v(n) + 0.5v(n - 2)$:} \nonumber\\
		&=
		\underset{\Delta \in \setNatural}{min}\ \E \bigg[ \big( v(n) + 0.5v(n-2) \big) \sum_{i=0}^{M-1} w_{i} \big( \eta(n - \Delta - i) \big) \bigg] \\
		&=
		\underset{\Delta \in \setNatural}{min}\ \E \bigg[ \big( v(n) + 0.5v(n-2) \big) \sum_{i=0}^{M-1} w_{i} \big( v(n - \Delta - i) + 0.5 v(n - 2 - \Delta - i) \big) \bigg]
		\label{eq: 2-3a-choose-delta} 
		\end{align}
		\noindent
		Since $v(n)$ is identically and \textbf{independently} distributed (i.i.d), \textit{zero mean} white noise:
		\begin{equation}
		\E \bigg[ v(n) v(n - j) \bigg] = 0, \quad \forall j \neq 0
		\label{eq: 2-3a-iid:helper}
		\end{equation}
		
		(\ref{eq: 2-3a-choose-delta}) is zero and minimised for $\Delta > 2$ and maximised for $\Delta = 0$, a direct consequence of (\ref{eq: 2-3a-iid:helper}).
		This is somewhat intuitive as the coloured noise signal $\eta(n)$ is a second order MA process. \newline
		\noindent
		MPSE is plotted against $\Delta$, Figure \ref{fig: 2-3a}, verifying the improved performance for $\Delta > 2$.
		
		\subsubsection{Adaptive Line Enhancer (ALE) Delay Investigation}
		\begin{minipage}[b]{0.49\textwidth}
%			\subsubsection{Adaptive Line Enhancer (ALE) Delay Investigation}
				Figure \ref{fig: 2-3a} shows the influence of ALE Delay on a range of model orders. We can see the minimum MPSE for Delay of 5 for Model Order 5. As we increase the delay we see a linear increase in log emphasised MPSE, letting us know that the samples that are more than 5 time points away are getting decreasingly uncorrelated to the current prediction, as expected for this second order moving average process.
%			\subsubsection{Adaptive Line Enhancer (ALE) Filter Order Investigation}
%				Figure \ref{fig: 2-3b} shows the influence of ALE Model Order for a given delay. This hyper parameter sweep is informative in understanding how many samples are needed to appropriately model the noise system, in this case we see that orders of 3 and 5 give us minimum MPSE. As we start to overfit with increasing model orders we get increasingly worse predictions.
		\end{minipage}% 
		\begin{minipage}{0.04\textwidth}
			\hspace*{0.04\textwidth}
		\end{minipage}% 
		\begin{minipage}[t]{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_3b_fig02.pdf} 
			\captionsetup{justification=centering}
			\captionof{figure}{ALE Delay \& Order Sweep}
			\label{fig: 2-3a}
%			\vspace*{10px}
%			\centering
%			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_3b_fig03.pdf} 
%			\captionsetup{justification=centering}
%			\captionof{figure}{ALE Order Sweep}
%			\label{fig: 2-3b}
		\end{minipage}%
	
	\subsubsection{Adaptive Line Enhancer (ALE) Filter Order Investigation}
		\begin{minipage}[b]{0.49\textwidth}
%			\subsubsection{Adaptive Line Enhancer (ALE) Delay Investigation}
			Figure \ref{fig: 2-3b} shows the influence of ALE Model Order for a given delay. This hyper parameter sweep is informative in understanding how many samples are needed to appropriately model the noise system, in this case we see that orders of 3 and 5 give us minimum MPSE. As we start to overfit with increasing model orders we get increasingly worse predictions.
		\end{minipage}% 
		\begin{minipage}{0.04\textwidth}
			\hspace*{0.04\textwidth}
		\end{minipage}% 
		\begin{minipage}[t]{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_3b_fig03.pdf} 
			\captionsetup{justification=centering}
			\captionof{figure}{ALE Order Sweep}
			\label{fig: 2-3b}
		\end{minipage}%
		
		
		\subsubsection{Adaptive Noise Cancellation (ANC) Comparison}
			\begin{figure}[H]
				\centering
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_3c_fig01.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_3c_fig02.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				\captionsetup{justification=centering}
				\caption{ALE and ANC compared, MSPE was calculated from time index 600+}
				\label{fig: 2-3c}
			\end{figure}
		
			To compare the ANC and ALE algorithms we fixed the Model Order to $M=6$ and set the ALE Delay to $\Delta=3$. This ALE Delay is one of the optimums as shown in Figure \ref{fig: 2-3a}. What we observe in Figure \ref{fig: 2-3c}, is the 15 times better MPSE performance of the ANC algorithm.\\
			When looking at the ensemble of predicted traces for both algorithms we see that the ANC stabilisation time is longer than that of ALE, but the stabilised prediction is much closer to the true signal. The mean trace shows us that there is both an amplitude error and phase error for the ALE predictor, which is not seen in the ALE predictor.
		
		\subsubsection{Adaptive Noise Cancellation (ANC) on real EEG Data}
			\begin{figure}[H]
				\centering
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 2.70cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_3d_fig01.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.8cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_3d_fig06.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				
				\captionsetup{justification=centering}
				\caption{ Original Spectrogram \& investigating hyperparameters. \\
						  50Hz was defined with a $\pm$2Hz window. }
				\label{fig: 2-3d-orig+paramSweep}
			\end{figure}
		
			As we can see above in Figure \ref{fig: 2-3d-orig+paramSweep}, a hyper parameter heatmap highlights the RMSE tradeoffs we can expect for varying model order, $M$, and learning rate, $\mu$, with this EEG dataset. Using this knowledge we selected $M=10,\mu=0.01$ as a satisfactory compromise. 
			
			\begin{figure}[H]
				\centering
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_3d_fig02.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 2.70cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_3d_fig03.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_3d_fig04.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 2.70cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q2_3d_fig05.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				\captionsetup{justification=centering}
				\caption{$\mu$'s effect on noise cancellation}
				\label{fig: 2-3d}
			\end{figure}
		
			In Figure \ref{fig: 2-3d}, we can see that using the selected parameters neatly seals off the 50Hz noise. When we investigate the influence of the greater learning rate, we see great attenuation around the 50Hz band spreading by 25Hz to the surrounding frequencies. Additionally, we can see an amplification in the harmonic of the 50Hz noise, at 100Hz. \\
			
			It is important to highlight the significance of the external noise fed into the ANC algorithm, testing showed that increasing the noisiness of the provided input noise signal drastically reduced ANC performance, outputs were unstable and rapidly approached infinity, becoming more prominent for larger $\mu$. In the example in Figure \ref{fig: 2-3d} a 50Hz signal was generated with noise power $\sigma_\eta^{ 2}=0.1$.\\
			 In technology where this algorithm would be useful, such as noise cancelling headphones, we would ensure that the noise external microphones pick up have a known linear relationship to what is heard inside the headphones. If this relationship is not approximately linear we can imagine that the noise cancellation effect would be less pronounced, unless other preprocessing is used before being fed into the ANC algorithm.
	
\pagebreak
\section{Widely Linear Filtering and Adaptive Spectrum Estimation} \label{sec: 3-WLASE}
	\subsection{Complex LMS and Widely Linear Modelling} \label{sec: 3-1-CLMS-ACLMS}
		\subsubsection{Widely Linear Moving Average (WLMA) Process using Complex LMS (CLMS) \& Augmented CLMS (ACLMS)}
			\vspace*{-\baselineskip}
			\begin{figure}[H]
				\centering
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_1a_fig01.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_1a_fig02.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				
				\captionsetup{justification=centering}
				\caption{Data Circularity's and the suitability of CLMS \& ACLMS to model a WLMA(1) }
				\label{fig: 3-1a}
			\end{figure}
			
			Figure \ref{fig: 3-1a}, shows the difference in circularity of white gaussian noise and the first order widely linear moving average process. Additionally, we can see the CLMS learning curve struggle to model the non-circular WLMA(1) process, converging to an MSPE $>0$dB.
			
		\subsubsection{CLMS and ACLMS Suitability with real (wind) data}
		\vspace*{-\baselineskip}
			\begin{figure}[H]
				\centering
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_1b_fig04.pdf} 
				\end{subfigure}
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_1b_fig05.pdf} 
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_1b_fig06.pdf} 
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_1b_fig07.pdf} 
				\end{subfigure}			
				\captionsetup{justification=centering}
				\captionof{figure}{Wind Regime Circularity and CLMS \& ACLMS estimates \\
					Low, Medium and High Regimes used $\mu=0.1,0.01,0.001$ respectively}
				\label{fig: 3-1b}
			\end{figure}
		
		Figure \ref{fig: 3-1b}, shows the circularity of the various wind regimes and how the CLMS and ACLMS fare at modelling the data. This dataset highlights the rotation invariant property, where the further the regime centroid is from the origin the higher the circularity. We can observe for high circularity, low $|\rho|$, that the CLMS MSPE minimises at a larger model order compared to ACLMS. However, once circularity decreases, high $|\rho|$, the two algorithms approximately agree on model order. ACLMS tends to outperform CLMS, but we can observe for  $|\rho| \approx 0.5$ with the Medium Regime the CLMS converges to a solution that provides the same MSPE minimum. We can finally note that ACLMS consistently obtains a minimum MPSE at Model Order 4, suggesting the order of the underlying data, and how the ACLMS overfitting causes it to perform worse than the CLMS for high moder orders.
	
		\subsubsection{Balanced and UnBalanced Voltages - Investigating Circularity}
		\vspace*{-\baselineskip}
			\begin{figure}[H]
				\centering
				\begin{subfigure}{0.45\textwidth}
					\centering
					\includegraphics[trim={2.2cm 6.8cm 3.00cm  6.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_1c_fig01.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.45\textwidth}
					\centering
					\includegraphics[trim={2.2cm 6.8cm 3.00cm  6.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_1c_fig02.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
			
				\begin{subfigure}{0.45\textwidth}
					\centering
					\includegraphics[trim={2.2cm 6.8cm 3.00cm  6.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_1c_fig03.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.45\textwidth}
					\centering
					\includegraphics[trim={2.2cm 6.8cm 3.00cm  6.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_1c_fig04.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
					
				\captionsetup{justification=centering}
				\caption{Voltage Data Circularity's and the influence of Phase and Magnitude Imbalance}
				\label{fig: 3-1c}
			\end{figure}
		
			Figure \ref{fig: 3-1c}, demonstrates that balanced voltages are purely circular whereas unbalanced voltages are ellipses, allowing us to clearly identify if there is fault, but may be difficult to discern which bias is in play. In both cases the magnitude of the bias is proportional to the circularity, $|\rho|$.  \\
			Performing a phase sweep we observe that phase bias reduces the radius of the ellipse's minor axis and a slight change to the major axis. \\
			In the voltage magnitude sweep we observe an increase of the ellipse's major axis radius and decrease of the minor axis radius. We can see a slight rotation about the origin as the ellipse aligns with its major axis.
			
		\subsubsection{Derivation for estimating Frequency from Filter Weights}
			\noindent
			\textbf{Balanced Complex Voltage Frequency} \newline
			\noindent
			Balanced complex $\alpha$$-$$\beta$ Clarke Voltages are defined with:
			\vspace*{-0.6\baselineskip}
			\begin{equation}
			v(n) = \sqrt{\frac{3}{2}} V e^{j(2\pi \frac{f_{o}}{f_{s}} n + \phi)}
			\end{equation}
			
			For time index $n+1$:
			\vspace*{-\baselineskip}
			\begin{align}
			v(n+1)  &= \sqrt{\frac{3}{2}} V e^{j(2\pi \frac{f_{o}}{f_{s}} (n+1) + \phi)} \\
			&= \sqrt{\frac{3}{2}} V e^{j(2\pi \frac{f_{o}}{f_{s}} n + \phi)} e^{j 2 \pi \frac{f_{o}}{f_{s}}} \\
			&= v(n) e^{j 2 \pi \frac{f_{o}}{f_{s}}}
			\label{eq: 3-1d-u_n+1}
			\end{align}
			\noindent
			The Strictly Linear autoregressive model of order 1, is defined as:
			\begin{equation}
			v(n+1) = h^{*}(n) v(n) 
			\end{equation}
			
			We can express coefficient $h^{*}(n)$ as a function of the voltage, then take it's conjugate to give us $h(n)$:
			\vspace*{-\baselineskip}
			\begin{align}
			 h^{*}(n) &= \frac{v(n+1)}{v(n)} = \frac{v(n) e^{j 2 \pi \frac{f_{o}}{f_{s}}}}{v(n)} \\
			 h^{*}(n) &= e^{j 2 \pi \frac{f_{o}}{f_{s}}} \\
			 h(n) &= e^{-j 2 \pi \frac{f_{o}}{f_{s}}}  \label{eq: 3-1d-h_balanced}\\
			&= | h(n) | e^{j\big( arctan \left\{\frac{\Im\{h(n)\}}{\Re\{h(n)\}} \right\} \big)} \label{eq: 3-1d-h_general}
			\end{align}
			\noindent
			The general complex form, (\ref{eq: 3-1d-h_general}), can only be equal to the derived form, (\ref{eq: 3-1d-h_balanced}), if both their magnitudes and their angles are equal, therefore we can equate and solve for $f_0$: 
			\begin{align}
			2 \pi \frac{f_{o}}{f_{s}} &= arctan \left\{\frac{\Im\{h(n)\}}{\Re\{h(n)\}} \right\} \\
			f_{o} &= \frac{f_{s}}{2\pi} arctan \left\{\frac{\Im\{h(n)\}}{\Re\{h(n)\}} \right\}
			\label{eq: 3-1d-fo_CLMS:proof}
			\end{align}
			
			\noindent
			\textbf{Unbalanced Complex Voltage Frequency} \\
			Unbalanced complex $\alpha$$-$$\beta$ Clarke Voltages are defined with:
			\vspace*{-0.5\baselineskip}
			\begin{equation}
			v(n) = A(n) e^{j(2\pi \frac{f_{o}}{f_{s}} n + \phi)} + B(n) e^{-j(2\pi \frac{f_{o}}{f_{s}} n + \phi)}
			\label{eq: 3-1d-u_unbalanced}
			\end{equation}
			
			The Widely Linear autoregressive model of order 1, is defined as:
			\vspace*{-0.5\baselineskip}
			\begin{equation}
			v(n+1) = h^{*}(n) v(n) + g^{*}(n) u^{*}(n)
			\end{equation}
			
			we substitute the Clarke Voltage Definition, (\ref{eq: 3-1d-u_unbalanced}):
			\vspace*{-0.8\baselineskip}
			\begin{align}
			v(n+1) =\ 
			&h^{*}(n) \bigg[ A(n) e^{j(2\pi \frac{f_{o}}{f_{s}} n + \phi)} + B(n) e^{-j(2\pi \frac{f_{o}}{f_{s}} n + \phi)} \bigg] \nonumber\\
			&+ g^{*}(n) \bigg[ A^{*}(n) e^{-j(2\pi \frac{f_{o}}{f_{s}} n + \phi)} + B^{*}(n) e^{j(2\pi \frac{f_{o}}{f_{s}} n + \phi)} \bigg]
			\label{eq: 3-1d-u_n+1_ar}
			\end{align}
			
			For time index $n+1$:
			\begin{equation}
			v(n+1) = A(n+1) e^{j(2\pi \frac{f_{o}}{f_{s}} (n+1) + \phi)} + B(n+1) e^{-j(2\pi \frac{f_{o}}{f_{s}} (n+1) + \phi)}
			\label{eq: 3-1d-u_n+1_clarke}
			\end{equation}
			
			Equating (\ref{eq: 3-1d-u_n+1_ar}) and (\ref{eq: 3-1d-u_n+1_clarke}), and by inspecting the common exponential terms:
			\begin{align}
			A(n+1) e^{j(2\pi \frac{f_{o}}{f_{s}} (n+1) + \phi)} &= \bigg[ h^{*}(n) A(n) + g^{*}(n) B^{*}(n) \bigg] e^{j(2\pi \frac{f_{o}}{f_{s}} n + \phi)} \\
			B(n+1) e^{-j(2\pi \frac{f_{o}}{f_{s}} (n+1) + \phi)} &= \bigg[ h^{*}(n) B(n) + g^{*}(n) A^{*}(n) \bigg] e^{-j(2\pi \frac{f_{o}}{f_{s}} n + \phi)}
			\end{align}
			
			Assuming that the amplitude change over time is negligible, $A(n+1) \approx A(n)$ and $B(n+1) \approx B(n)$, the equations simplify as follows:
			\begin{align}
			e^{j 2\pi \frac{f_{o}}{f_{s}}} = \frac{h^{*}(n) A(n) + g^{*}(n) B^{*}(n)}{A(n+1)} \approx h^{*}(n) + g^{*}(n) \frac{B^{*}(n)}{A(n)} \label{eq: 3-1d-e_A}\\
			e^{-j 2\pi \frac{f_{o}}{f_{s}}} = \frac{h^{*}(n) B(n) + g^{*}(n) A^{*}(n)}{B(n+1)} \approx h^{*}(n) + g^{*}(n) \frac{A^{*}(n)}{B(n)} \label{eq: 3-1d-e_B}
			\end{align}
			
			As (\ref{eq: 3-1d-e_A}) is the complex conjugate of (\ref{eq: 3-1d-e_B}):
			\begin{align}
			h^{*}(n) + g^{*}(n) \frac{B^{*}(n)}{A(n)} = h(n) + g(n) \frac{A(n)}{B^{*}(n)}
			\end{align}
			
			Multiplying with $\frac{B^{*}(n)}{A(n)}$ both sides:
			\begin{align}
			\bigg( h^{*}(n) - h(n) \bigg) \frac{B^{*}(n)}{A(n)} + g^{*}(n) \bigg( \frac{B^{*}(n)}{A(n)} \bigg)^{2} - g(n) = 0
			\label{eq: 3-1d-quadratic}
			\end{align}
			
			We can solve the quadratic (\ref{eq: 3-1d-quadratic}), to find $\frac{B^{*}(n)}{A(n)}$:
			\begin{align}
			\frac{B^{*}(n)}{A(n)}   &= \frac{- \bigg( h^{*}(n) - h(n) \bigg) \pm \sqrt{\bigg( h^{*}(n) - h(n) \bigg)^{2} + 4 g^{*}(n) g(n)}}{2g^{*}(n)} \\
			&= \frac{2 j\Im\{h(n)\} \pm j\sqrt{-4 \Im\{h(n)\}^{2} + 4 |g(n)|^{2}}}{2g^{*}(n)} \\
			&= \frac{j\Im\{h(n)\} \pm j\sqrt{\Im\{h(n)\}^{2} - |g(n)|^{2}}}{g^{*}(n)}
			\end{align}
			
			Substituting this result into (\ref{eq: 3-1d-e_A}):
			\begin{align}
			e^{j 2\pi \frac{f_{o}}{f_{s}}}  &= h^{*}(n) + \Im\{h(n)\}j \pm j\sqrt{\Im\{h(n)\}^{2} - |g(n)|^{2}} \\
			&= \Re\{h(n)\} \pm j\sqrt{\Im\{h(n)\}^{2} - |g(n)|^{2}} \\
			\end{align}
			
			Keeping the positive solution, since $f_{s} \gg f_{o} > 0$:
			\begin{align}
			e^{j 2\pi \frac{f_{o}}{f_{s}}}  &= \Re\{h(n)\} + j\sqrt{\Im\{h(n)\}^{2} - |g(n)|^{2}} \\
			&= \rho e^{j\big(\frac{\sqrt{\Im\{h(n)\}^{2} - |g(n)|^{2}}}{\Re\{h(n)\}}\big)} \label{eq: 3-1d-h_unbalanced}
			\end{align}
			
			where $\rho > 0$. \\
			\noindent
			Equating the generic, (\ref{eq: 3-1d-h_general}), and derived form, (\ref{eq: 3-1d-h_unbalanced}), then solving for $f_{o}$:
			\begin{equation}
			f_{o} = \frac{f_{s}}{2 \pi} arctan \left\{\frac{\sqrt{\Im\{h(n)\}^{2} - |g(n)|^{2}}}{\Re\{h(n)\}}\right\}
			\label{eq: 3-1d-fo_ACLMS:proof}
			\end{equation}
		\subsubsection{Investigating the Accuracy of the Filter Estimates}
			\begin{figure}[H]
				\centering
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_1e_fig01.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_1e_fig02.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_1e_fig03.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_1e_fig04.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				
				\captionsetup{justification=centering}
				\caption{CLMS \& ACLMS Estimates of Balanced \& Unbalanced Voltage Data}
				\label{fig: 3-1e}
			\end{figure}
		
			In estimating a 50Hz mains noise we see that in the balanced case both algorithms converge to the correct estimate. \\
			In the unbalanced case we see that the CLMS algorithm has troubles converging and fluctuates about the correct estimate. The ACLMS tends to converges to the correct estimate, we do note there are fluctuations in the error and in the time frame simulated and that convergence has not been reached.\\
			The discrepancy between CLMS and ACLMS in estimating the balanced and unbalanced voltages lies in the CLMS being unable to widely linear nature of the voltage and is not guaranteed to capture the complete second-order statistical relationship, as they work best with data with rotation invariant probability distributions. As was shown in Figure \ref{fig: 3-1c}, the unbalanced voltages are no longer rotationally invariant once they form the ellipsoid (non-circular) shape.
	\subsection{Adaptive AR Model Based Time-Frequency Estimation} \label{sec: 3-2-adaptive-ar-spectrum-estimate}
		\subsubsection{Frequency Modulated (FM) Signal Investigated with an AR(1) Process}
			\begin{figure}[H]
				\centering
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_2a_fig01.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_2a_fig02.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				
				\captionsetup{justification=centering}
				\caption{Input Frequency \& Phase Spectrums}
				\label{fig: 3-2a}
			\end{figure}
			\vspace*{-\parskip}
			\begin{figure}[H]
				\centering
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_2a_fig03.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_2a_fig04.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				
				\captionsetup{justification=centering}
				\caption{AR modelling of Input Data}
				\label{fig: 3-2a-AR-models}
			\end{figure}
			Figure \ref{fig: 3-2a-AR-models}, highlights how AR models of varying order cannot capture the frequency variations of the signal simulated, shown in Figure \ref{fig: 3-2a}. If we attempt to segment the signal into its piecewise parts, we see the AR(1) model only accurately identifies the 100Hz response.
			
		\subsubsection{Frequency Modulated (FM) Signal Investigated with CLMS}
			\begin{figure}[H]
				\centering
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_2b_fig02.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_2b_fig04.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				
				\captionsetup{justification=centering}
				\caption{CLMS Spectrograms}
				\label{fig: 3-2b}
			\end{figure}
		
			\begin{minipage}[b]{0.49\textwidth}
				As shown in Figure \ref{fig: 3-2b}, we are able to better reconstruct the frequency variations using the CLMS algorithm. We must stress the importance of selecting the right $\mu$ value, we observe that we have a poor fitting curve for $\mu<0.1$. An interesting observation is the outlier \% of the total number of elements as we change $\mu$, shown in Figure \ref{fig: 3-2b-outliers}, we see it reaches a maximum between $\mu=0.05$ to $0.1$, corresponding to our optimum $\mu$ value range.
			\end{minipage}% 
			\begin{minipage}{0.04\textwidth}
				\hspace*{0.04\textwidth}
			\end{minipage}% 
			\begin{minipage}[t]{0.49\textwidth}
				\centering
				\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_2b_fig06.pdf} 

				\captionsetup{justification=centering}
				\captionof{figure}{Outliers for Increasing $\mu$}
				\label{fig: 3-2b-outliers}
			\end{minipage}%
	\subsection{A Real Time Spectrum Analyser Using Least Mean Square} \label{sec: 3-3-real-time-spectrum-LMS}
		\subsubsection{The Discrete Fourier Transform (DFT) Formula and the Least Square Solution}
			We know that any signal, $y(n)$, can be expressed as a weighted sum of $N$ harmonically related complex exponentials:
			\begin{equation}
			\hat{y}(n) = \sum_{k=0}^{N-1}w(k)e^{j\frac{2\pi kn}{N}}
			\end{equation}
			
			\noindent
			We can collect all $w(k)$ weights into a vector, $\vw$, and similarly all complex exponentials into a symmetric matrix, $\mF$. \\
			The predicted signal vector is thereby defined as:
			\begin{equation}
			\hat{\vy} = \mF\vw
			\label{eq: 3-3a-y_hat}
			\end{equation}
			
			\noindent
			The Least Squares framework minimises the sum of square error between the true signal $y(n)$ and the estimated signal $\hat{y}(n)$:
			\begin{equation}
			\underset{\vw}{min} ||\vy-\hat{\vy}||^2
			\label{eq: 3-3a-ls-sqrErr}
			\end{equation}
			
			\noindent
			This minimisation can be done with the gradient descent algorithm, by using the negative gradient of, (\ref{eq: 3-3a-ls-sqrErr}).\\
			The absolute minimum being $0$, we can solve for $\vw$:
			\begin{align}
			0 					&= \nabla_{\vw} \bigg( \underset{\vw}{min} ||\vy-\hat{\vy}||^2 \bigg) \\
			 					&= \nabla_{\vw} \bigg( (\vy-\mF\vw)^{H} (\vy-\mF\vw) \bigg) \\
								&= \nabla_{\vw} \bigg( \vy^{H}\vy-\vw^H\mF\vy -\vy^{H}\mF\vw+\vw^{H}\mF^{H}\mF\vw \bigg)\\
								&= -2\vy^{H}\mF + 2\vw^{H}\mF^{H}\mF\\
			\vw^{H}\mF^{H}\mF	&= \vy^{H}\mF \\
			\mF^{H}\mF\vw		&= \mF^{H}\vy \\
			\vw					&= (\mF^{H}\mF)^{-1}\mF^{H}\vy \label{eq: 3-3a-weight:proof}
			\end{align}
			
					
		\subsubsection{Fourier Transform in terms of Changes of Basis \& Projections}
			As highlighted by equation (\ref{eq: 3-3a-weight:proof}), the Fourier coefficients, i.e. $\vw_{1\times N}$, are a linear combination of the (orthonormal) columns of the matrix $\mF_{N\times N}$.
			
			The DFT operation can be interpreted as a projection of the time-domain input vector, $\vy$, into the frequency domain, i.e. the subspace defined by the $\mF$ matrix. The columns are the basis of this new (frequency) domain, with each column representing $N$ sinusoids, each a multiple of the frequency $\frac{f_{s}}{N}$, where $f_{s}$ the sampling frequency.
			
		\subsubsection{The DFT-CLMS vs. the AR(1) at Spectrum Analysis}
		
			\begin{minipage}[b]{0.49\textwidth}
				Figure \ref{fig: 3-3c-no-gamma}, is the spectrogram of the required DFT-CLMS. We can see a trail effect, where a frequency component is carried in time. Hence the reason this spectrogram does not match the earlier spectrogram is because the algorithm has not forgotten the previous frequency components of our non-linear frequency system. By implementing a leakage, forgetfulness, factor $\gamma$, we find that the CLMS can better model the underlying data.
			\end{minipage}% 
			\begin{minipage}{0.04\textwidth}
				\hspace*{0.04\textwidth}
			\end{minipage}% 
			\begin{minipage}[t]{0.49\textwidth}
				\centering
				\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_3c_fig01.pdf} 
				
				\captionsetup{justification=centering}
				\captionof{figure}{Effect of No $\gamma$}
				\label{fig: 3-3c-no-gamma}
			\end{minipage}%
		
			\begin{figure}[H]
				\centering
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_3c_fig02.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_3c_fig03.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
			
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_3c_fig04.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 3.00cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_3c_fig05.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				
				\captionsetup{justification=centering}
				\caption{DFT-CLMS Spectrograms}
				\label{fig: 3-3c}
			\end{figure}
			
			
		\subsubsection{The DFT-CLMS on real (EEG) Data}
			
			\begin{figure}[H]
				\centering
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 2.90cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_3d_fig01.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 2.90cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_3d_fig02.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 2.90cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_3d_fig03.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				%		~ % forces onto the same row
				\begin{subfigure}{0.49\textwidth}
					\centering
					\includegraphics[trim={2.2cm 11.2cm 2.90cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q3_3d_fig04.pdf} 
					\captionsetup{justification=centering}
				\end{subfigure}
				
				\captionsetup{justification=centering}
				\caption{DFT-CLMS Spectrograms. Red lines have been plotted at: 13, 26, 50 \& 100 Hz }
				\label{fig: 3-3d}
			\end{figure}
			
			In looking at EEG data we now find the required DFT-CLMS works better than its forgetful implementations. Frequency bands are more refined, we can better identify the SSVEP at 13 and 26Hz, the mains noise and its harmonics at 50 and and 100Hz and the alpha rhythm frequencies in the sub 10Hz range.\\
			Why the $\gamma$ factor does not improve the spectrogram accuracy can be because in the brain these frequencies are always present, hence as the algorithm starts forgetting, the error term rises and it attempts to re-learn these weights, this prevents the weights converging nicely as we see with the $\gamma=0$ spectrogram.
	
\pagebreak
\section{From LMS to Deep Learning} \label{sec: 4-LMS-to-DL}

		\begin{minipage}[t]{0.45\textwidth}
			\begin{table}[H]
				\centering
				\begin{tabular}{|c|c|c|}
					\hline
					&&\\[-1em]
					\textbf{Section} & \textbf{MSE} & \textbf{$R_p = 20\log_{10}({\frac{\hat{\sigma}_y}{\sigma_e}})$} \\
					&&\\[-1em]
					\hline
					\hline
					\ref{sec: 4-1-LMS}	&	40.102		&	5.1957 \\
					\hline
					\ref{sec: 4-2-dynamic-perc}	&	196.74	&	-23.190 \\
					\hline
					\ref{sec: 4-3-amplitude-dynamic-perc}	&	8.4374		&	13.928 \\
					\hline
					\ref{sec: 4-4-biasing-dynamic-perceptron} 	&	\makecell{ Bias: 10 \\ 11.796 \\ Bias: 1 \\ 13.600 \\}		&	\makecell{ Bias: 10 \\ 12.645 \\ Bias: 1 \\ 12.275 \\} \\
					\hline
					\ref{sec: 4-5-weight-training-dynamic-perceptron}	&	10.360		&	14.174 \\
					\hline
				\end{tabular}
				\captionsetup{justification=centering}
				\caption{Mean Square Errors \& Prediction Gains for Sections 1-5}
				\label{tab: 4-mse-Rp}
			\end{table}
		\end{minipage}% 
		\begin{minipage}{0.08\textwidth}
			\hspace*{0.08\textwidth}
		\end{minipage}% 
		\begin{minipage}[t]{0.45\textwidth}
			\centering
			\begin{table}[H]
				\centering
				\begin{tabular}{|c|c|c|}
					\hline
					&&\\[-1em]
					\textbf{Section} & \textbf{MSE} & \textbf{$R_p = 20\log_{10}({\frac{\hat{\sigma}_y}{\sigma_e}})$} \\
					&&\\[-1em]
					\hline
					\hline
					\ref{sec: 4-1-LMS}	&	15.610		&	9.8044 \\
					\hline
					\ref{sec: 4-2-dynamic-perc}	&	158.53	&	-22.067 \\
					\hline
					\ref{sec: 4-3-amplitude-dynamic-perc}	&	5.1132		&	15.200 \\
					\hline
					\ref{sec: 4-4-biasing-dynamic-perceptron} 	&	\makecell{ Bias: 10 \\ 6.1193 \\ Bias: 1 \\ 14.660 \\}		&	\makecell{ Bias: 10 \\ 14.582 \\ Bias: 1 \\ 15.3295 \\} \\
					\hline
					\ref{sec: 4-5-weight-training-dynamic-perceptron}	&	8.7107		&	14.134 \\
					\hline
				\end{tabular}
				\captionsetup{justification=centering}
				\caption{Mean Square Errors \& Prediction Gains for Sections 1-5. Taken from Index 500+, to remove transient effects}
				\label{tab: 4-mse-Rp-500+}
			\end{table}
		\end{minipage}%
		
	
	\subsection{The LMS} \label{sec: 4-1-LMS}
		\begin{minipage}[b]{0.49\textwidth}
			In this detrended data we see the classic (linear) LMS convergence, where the amplitude starts small due to the weights initialised to zero. We consistently see the algorithm undershooting for this particular dataset, Figure \ref{fig: 4-1}. Ignoring the outlier from \ref{sec: 4-2-dynamic-perc}: We note that LMS has the lowest $R_p$ value both for the entire and weight stabilised data segments, this tells us that the ratio of standard deviations is closest to 1 - showing us the variation of the predicted signal and the prediction error are  most similar.
		\end{minipage}% 
		\begin{minipage}{0.04\textwidth}
			\hspace*{0.04\textwidth}
		\end{minipage}% 
		\begin{minipage}[b]{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q4_1_fig01.pdf} 
			\captionsetup{justification=centering}
			\captionof{figure}{Linear LMS}
			\label{fig: 4-1}
		\end{minipage}%
	
		When looking at the change from entire to weight stabilised data we can see: the LMS algorithm has the longest weight stabilisation time, $\approx$250 steps, reflected in it having the largest differences in MSE error and $R_p$.
	\subsection{The Dynamic Perceptron with $\texttt{tanh}$ Activation} \label{sec: 4-2-dynamic-perc}
		\begin{minipage}[b]{0.49\textwidth}
			In this non-linear LMS convergence, the \texttt{tanh} function ensures that the output is constrained to $\pm1$, this is clearly seen in Figure \ref{fig: 4-2}, with the very small amplitude. What we also observe when we look closer is the function reaches a stable prediction of sign changes of the input signal in 20 steps. The negative $R_p$ is also a consequence of the constrained \texttt{tanh} range. As the prediction matches the fluctuation trend of the data well  and the non-linearity of the activation function may help model the non-linearity of the data, we can argue that this activation is suitable for this dataset. 
		\end{minipage}% 
		\begin{minipage}{0.04\textwidth}
			\hspace*{0.04\textwidth}
		\end{minipage}% 
		\begin{minipage}[b]{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q4_2_fig01.pdf} 
			\captionsetup{justification=centering}
			\captionof{figure}{Dynamic Perceptron, Non-Linear LMS with \texttt{tanh} Activation}
			\label{fig: 4-2}
		\end{minipage}%
	
	
	\subsection{Adding Amplitude Scaling to the Dynamic Perceptron} \label{sec: 4-3-amplitude-dynamic-perc}
		\begin{minipage}[b]{0.49\textwidth}
			By now scaling the magnitude of the output of the \texttt{tanh} activation we observe that the prediction now matches the true signal with much better accuracy than even that stabilised LMS. As the output range of \texttt{tanh} is $\pm1$, the amplitude scaling must be at least the maximum value observed in the true signal: $a\in[\texttt{y},\inf]$ By increasing this value we would no longer need to utilise the full $\pm1$ range of \texttt{tanh}, this can be seen as beneficial as the closer we reach these limits the more non-linear the scaling becomes.
		\end{minipage}% 
		\begin{minipage}{0.04\textwidth}
			\hspace*{0.04\textwidth}
		\end{minipage}% 
		\begin{minipage}[b]{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q4_3_fig01.pdf} 
			\captionsetup{justification=centering}
			\captionof{figure}{Dynamic Perceptron with Constant Amplitude Scaling}
			\label{fig: 4-3}
		\end{minipage}%
	
		The $R_p$ value is now higher than the LMS for both the entire and stabilised data ranges. This means the prediction variance is larger than our error variance, this can be interpreted as the impact of the non-linear scaling.
		
	\subsection{Biasing with the Dynamic Perceptron} \label{sec: 4-4-biasing-dynamic-perceptron}
		\begin{minipage}[b]{0.49\textwidth}
			Bias here can be interpreted as the sensitivity to changes in the (error) signal power, the bias value can thereby be increased to increase the sensitivity to variation in power. Using a bias of 10 instead of the proposed 1 improved MSE significantly, especially during the weight stabilised region. When looking at the convergence we see that the prediction keeps increasing in magnitude until it reaches the true signal before it begins the zero-mean prediction observed before.
		\end{minipage}% 
		\begin{minipage}{0.04\textwidth}
			\hspace*{0.04\textwidth}
		\end{minipage}% 
		\begin{minipage}[b]{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q4_4_fig01.pdf} 
			\captionsetup{justification=centering}
			\captionof{figure}{Dynamic Perceptron with Auto-biasing}
			\label{fig: 4-4}
		\end{minipage}%
	
		We can see from the MSE and $R_p$ values that the prediction is generally less accurate but has similar prediction gains. Although we should note that for a bias of 10 the prediction gain is smaller than for a bias of 1 and 0.
	\subsection{Pre-training the weights} \label{sec: 4-5-weight-training-dynamic-perceptron}
		\begin{minipage}[b]{0.49\textwidth}
			Uses a single set of pre-trained weights for the entire dataset shows rapid convergence to the data mean, but we can see trade-offs by no longer using an adaptive approach, such as the increase in overshoot. This overshoot does not prevent it from outperforming the adaptive approach in MSE. Another interesting point to note is similarity of $R_p$ to the stabilised values of a large bias - suggesting the possibility to reduce the number of epochs by using a larger bias.
		\end{minipage}% 
		\begin{minipage}{0.04\textwidth}
			\hspace*{0.04\textwidth}
		\end{minipage}% 
		\begin{minipage}[b]{0.49\textwidth}
			\centering
			\includegraphics[trim={2.2cm 11.2cm 3.15cm  11.2cm}, clip, width=\textwidth]{../MATLAB/figures/q4_5_fig01.pdf} 
			\captionsetup{justification=centering}
			\captionof{figure}{Dynamic Perceptron with Pre-trained Weights}
			\label{fig: 4-5}
		\end{minipage}%
	\subsection{What is backpropogation?} \label{sec: 4-6-backpropogation}
		Backpropogation is an algorithm used to refine the weights used in a neural network. Its fundamental mathematic background lies within gradient descent.\\
		The first concept we must define is a `cost' function, ${C}()$. In the simplest case the cost function input is: a vector of weights (and biases) that correspond to each perceptron connection - $\vw$ - and a training data set, the cost function then defines mean of the square error between the desired output and the predicted output across the training samples. This cost function has an assumed minimum at $0$, which represents when both outputs are closely matched. We explicitly state the input is the set of perceptron connection weights and/or biases and the output is the mean square error for the training dataset: ${C}(\vw)=\texttt{MSE}$\\
		Gradient descent is the algorithm that looks to follow the negative gradient of a function, thereby finding the function \textbf{local minimum}. In this application, this means we are looking for changes to the weights in the neural network, that cause a negative change to the cost. \\
		The backpropogation algorithm gives us the negative gradient of the cost function to follow, $-\nabla{C}$, which tells us how we should change the weight or bias of each connected perceptron, $\vw$ to efficiently find the local minimum. Each element's magnitude represents how much the weight/bias contributes to decreasing the cost function; the sign indicates how the weight/bias should change, for example a large positive value indicates that increasing this weight/bias greatly decreases the cost function.
		
	\subsection{Deep Network Epoch Effect} \label{sec: 4-7-DL-epoch}
		\vspace*{-0.8\baselineskip}
		\begin{figure}[H]
			\centering
			\begin{subfigure}{\textwidth}
				\centering
				\includegraphics[width=\textwidth]{../Python/figures/Q4_LMS2DL_4_7+8_05.pdf} 
				\captionsetup{justification=centering}
			\end{subfigure}
			%		~ % forces onto the same row
			\begin{subfigure}{\textwidth}
				\centering
				\includegraphics[width=\textwidth]{../Python/figures/Q4_LMS2DL_4_7+8_17.pdf} 
				\captionsetup{justification=centering}
			\end{subfigure}

			\captionsetup{justification=centering}
			\caption{Learning Curves: 20,000 epochs, learning rate 0.01 and noise power 0.05}
			\label{fig: 4-7-LC-e20k-mu0.01-n0.05}
		\end{figure}
		
		Comparing the deep network with the single dynamical perceptron, Figure \ref{fig: 4-7-LC-e20k-mu0.01-n0.05}, the deep network is better at capturing the high frequency details in the signal, whereas the dynamical perceptron has a low pass effect of the signal, seeming to capture the signal mean. This likely contributes to the lower converged error observed in the deep network.
		We can additionally observe that the deep network requires a larger number of epoch steps before convergence, on both train and test data.
		
		The number of epochs was also varied, and included in the attached Jupyter Notebook although outside the question scope, with the following observations:\\
		
		For all algorithms: increasing the number of epochs tends to decrease the Test Loss. This makes sense and is equivalent to overfitting to the train data, which we see negatively impacts Test Loss for much larger epoch numbers, the algorithms most affects by this overfitting in order are: the Non-Linear LMS, Deep Network and finally the Linear LMS. When the number of epochs is too small the weights were not able to converge and provided suboptimal fitting to even the training signal magnitude.
		% TODO: The figures? Show point they kick back up
		Linear LMS: Increasing the number of epochs also decreases the time to convergence until it reaches some minimum convergence time, representing the fixed learning rate.
		Deep network: we observe that there is a phase shift implying the algorithm is predicting the signal.

	\subsection{Deep Network Noise Effect} \label{sec: 4-7-DL-noise}
		\vspace*{-0.8\baselineskip}
		\begin{figure}[H]
			\centering
			\begin{subfigure}{\textwidth}
				\centering
				\includegraphics[width=\textwidth]{../Python/figures/Q4_LMS2DL_4_7+8_06.pdf} 
				\captionsetup{justification=centering}
			\end{subfigure}
			%		~ % forces onto the same row
			\begin{subfigure}{\textwidth}
				\centering
				\includegraphics[width=\textwidth]{../Python/figures/Q4_LMS2DL_4_7+8_21.pdf} 
				\captionsetup{justification=centering}
			\end{subfigure}
			
			\captionsetup{justification=centering}
			\caption{Learning Curves: 20,000 epochs, learning rate 0.01 and noise power 0.1}
			\label{fig: 4-7-LC-e20k-mu0.01-n0.1}
		\end{figure}
		For all algorithms: increasing noise power increases Test Loss and increases the number of epochs required for convergence. Increasing the noise power also increases the likelihood that the Test Loss will not converge at all, as shown in Figure \ref{fig: 4-7-LC-e20k-mu0.01-n0.1}. This suggests that there is some characteristic of the data that is poorly modelled, reflected by how the dynamical perceptron has a sharper error gradient than the deep network. We can also observe that the deep network trace is less predictive than we saw with lower noise power in Figure \ref{fig: 4-7-LC-e20k-mu0.01-n0.05}. \\
		Testing beyond the scope of the question, in the attached Jupyter Notebook, found that greater epochs were able to better model the test data in \textit{low noise power regimes}, as noise power increased greater epochs simply overfitted to the test data and provided non-converging test errors.
		
	
\section{Tensor Decompositions for Big Data Applications} \label{sec: 5-TD-BD}
	NOTE: The interpretation of instructions has been made according to the images provided, hence a mode-1 as referred to in the images, although technically a mode-0 using Python's 0 indexing, was kept at mode-1.\\
	
	Answers can be found in the attached .html files and Jupyter Notebooks.
	
\end{document}

